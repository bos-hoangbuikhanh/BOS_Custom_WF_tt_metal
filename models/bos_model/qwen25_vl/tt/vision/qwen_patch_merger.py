"""
This is the patch merger implementation used in the Qwen-VL-7B model.

There's no existing implementation for this in tt_transformers,
so it was written specifically based on Qwen-VL's architecture.
"""

import time

import ttnn
from models.bos_model.qwen25_vl.tt.vision.qwen_rmsnorm import RMSNorm


class TTQwen2_5_VLPatchMerger:
    def __init__(
        self,
        device,
        dim,
        state_dict,
        weight_key,
        args,
        layer_num=None,
        state_dict_prefix="",
        weight_cache_path=None,
        weight_memory_config=ttnn.DRAM_MEMORY_CONFIG,
        weight_dtype=ttnn.bfloat8_b,
        is_distributed=None,
        eps: float = 1e-06,
        dims=3584,
        context_dim=1280,
        spatial_merge_size=2,
        mode="decode",
    ):
        super().__init__()
        self.eps = eps
        self.mode = mode

        self.args = args

        weight_name_1 = f"{state_dict_prefix}{weight_key}ln_q.weight"
        weight_name_2 = f"{state_dict_prefix}{weight_key}feed_forward.0.weight"
        weight_name_3 = f"{state_dict_prefix}{weight_key}feed_forward.2.weight"

        cache_name = lambda name: weight_cache_path / (f"{name}")

        self.weight_1 = ttnn.as_tensor(
            state_dict[weight_name_1],
            device=device,
            dtype=weight_dtype,
            mesh_mapper=ttnn.ShardTensorToMesh(device, dim=-1),
            layout=ttnn.TILE_LAYOUT,
            memory_config=weight_memory_config,
            cache_file_name=cache_name(weight_name_1),
        )

        self.weight_2 = ttnn.as_tensor(
            state_dict[weight_name_2].transpose(0, 1),
            device=device,
            dtype=weight_dtype,
            mesh_mapper=ttnn.ShardTensorToMesh(device, dim=-1),
            layout=ttnn.TILE_LAYOUT,
            memory_config=weight_memory_config,
            cache_file_name=cache_name(weight_name_2),
        )

        self.weight_3 = ttnn.as_tensor(
            state_dict[weight_name_3].transpose(0, 1),
            device=device,
            dtype=weight_dtype,
            mesh_mapper=ttnn.ShardTensorToMesh(device, dim=-1),
            layout=ttnn.TILE_LAYOUT,
            memory_config=weight_memory_config,
            cache_file_name=cache_name(weight_name_3),
        )

        self.hidden_size = context_dim * (spatial_merge_size**2)
        self.ln_q = RMSNorm(
            device=device,
            dim=1280,
            state_dict=state_dict,
            state_dict_prefix="",
            weight_key="visual.merger.ln_q",
            weight_dtype=ttnn.bfloat8_b,
            is_distributed=False,
            sharded_program_config=self.args.get_model_config()["SHARDED_NORM_ATTN_PRGM_CFG"],
            sharded_output_config=False,
        )

        self.compute_kernel_config = ttnn.WormholeComputeKernelConfig(
            math_fidelity=ttnn.MathFidelity.HiFi4,
            fp32_dest_acc_en=True,
            packer_l1_acc=True,
            dst_full_sync_en=False,
        )

    def __call__(self, x):
        x = self.ln_q(x, mode=self.mode)
        x = ttnn.reshape(x, (1, 1, -1, self.hidden_size))

        x = ttnn.linear(
            x,
            self.weight_2,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            compute_kernel_config=self.compute_kernel_config,
            activation="gelu",
        )

        if self.args.num_devices > 1:
            x = ttnn.all_gather(x, dim=3)

        # x = ttnn.gelu(x) # Fused with linear above

        x = ttnn.linear(
            x,
            self.weight_3,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            compute_kernel_config=self.compute_kernel_config,
        )

        if self.args.num_devices > 1:
            x = ttnn.all_gather(x, dim=3, num_links=1)

        x = ttnn.reshape(x, (-1, x.shape[-1]))

        return x
