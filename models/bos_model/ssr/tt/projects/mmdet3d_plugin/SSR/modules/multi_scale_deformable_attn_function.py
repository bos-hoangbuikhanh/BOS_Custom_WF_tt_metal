# TODO: Implement multi_scale_deformable_attn_pytorch function in ttnn
# This should be the work-around version for ttnn.bos_deformable_attention,
# which is currently under development.
